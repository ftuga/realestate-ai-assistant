[2025-04-17T03:20:59.079+0000] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: zephyr_real_estate_model_dag.download_model manual__2025-04-17T03:20:52.163358+00:00 [queued]>
[2025-04-17T03:20:59.083+0000] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: zephyr_real_estate_model_dag.download_model manual__2025-04-17T03:20:52.163358+00:00 [queued]>
[2025-04-17T03:20:59.083+0000] {taskinstance.py:1331} INFO - Starting attempt 1 of 4
[2025-04-17T03:20:59.090+0000] {taskinstance.py:1350} INFO - Executing <Task(PythonOperator): download_model> on 2025-04-17 03:20:52.163358+00:00
[2025-04-17T03:20:59.093+0000] {standard_task_runner.py:57} INFO - Started process 94 to run task
[2025-04-17T03:20:59.095+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'zephyr_real_estate_model_dag', 'download_model', 'manual__2025-04-17T03:20:52.163358+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/Download_model.py', '--cfg-path', '/tmp/tmpjp1upq50']
[2025-04-17T03:20:59.095+0000] {standard_task_runner.py:85} INFO - Job 5: Subtask download_model
[2025-04-17T03:20:59.121+0000] {task_command.py:410} INFO - Running <TaskInstance: zephyr_real_estate_model_dag.download_model manual__2025-04-17T03:20:52.163358+00:00 [running]> on host 11c3ec6fad98
[2025-04-17T03:20:59.156+0000] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='zephyr_real_estate_model_dag' AIRFLOW_CTX_TASK_ID='download_model' AIRFLOW_CTX_EXECUTION_DATE='2025-04-17T03:20:52.163358+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-04-17T03:20:52.163358+00:00'
[2025-04-17T03:20:59.156+0000] {Download_model.py:90} INFO - PyTorch patched for compatibility
[2025-04-17T03:20:59.254+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.9/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

[2025-04-17T03:21:00.172+0000] {Download_model.py:101} INFO - Downloading tokenizer: HuggingFaceH4/zephyr-7b-beta
[2025-04-17T03:21:00.401+0000] {Download_model.py:108} INFO - Downloading model: HuggingFaceH4/zephyr-7b-beta
[2025-04-17T03:21:01.067+0000] {logging_mixin.py:149} WARNING - Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]
[2025-04-17T03:21:01.267+0000] {logging_mixin.py:149} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.267+0000] {file_download.py:1670} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.270+0000] {logging_mixin.py:149} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.270+0000] {file_download.py:1670} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.281+0000] {logging_mixin.py:149} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.281+0000] {file_download.py:1670} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.283+0000] {logging_mixin.py:149} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.281+0000] {file_download.py:1670} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.285+0000] {logging_mixin.py:149} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.285+0000] {file_download.py:1670} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.288+0000] {logging_mixin.py:149} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.288+0000] {file_download.py:1670} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.289+0000] {logging_mixin.py:149} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.289+0000] {file_download.py:1670} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.313+0000] {logging_mixin.py:149} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:21:01.313+0000] {file_download.py:1670} WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[2025-04-17T03:23:39.598+0000] {logging_mixin.py:149} WARNING - Fetching 8 files:  12%|#2        | 1/8 [02:38<18:29, 158.53s/it]
[2025-04-17T03:25:07.069+0000] {logging_mixin.py:149} WARNING - Fetching 8 files:  25%|##5       | 2/8 [04:06<11:40, 116.73s/it]
[2025-04-17T03:25:42.152+0000] {logging_mixin.py:149} WARNING - Fetching 8 files:  38%|###7      | 3/8 [04:41<06:37, 79.45s/it] 
[2025-04-17T03:26:04.948+0000] {logging_mixin.py:149} WARNING - Fetching 8 files:  75%|#######5  | 6/8 [05:03<01:03, 31.90s/it]
[2025-04-17T03:26:04.948+0000] {logging_mixin.py:149} WARNING - Fetching 8 files: 100%|##########| 8/8 [05:03<00:00, 37.99s/it]
[2025-04-17T03:26:05.021+0000] {logging_mixin.py:149} WARNING - Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]
[2025-04-17T03:26:05.031+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '

[2025-04-17T03:26:05.131+0000] {logging_mixin.py:149} WARNING - Loading checkpoint shards:  50%|#####     | 4/8 [00:00<00:00, 36.78it/s]
[2025-04-17T03:26:05.196+0000] {logging_mixin.py:149} WARNING - Loading checkpoint shards: 100%|##########| 8/8 [00:00<00:00, 46.02it/s]
[2025-04-17T03:26:05.722+0000] {Download_model.py:119} INFO - Saving model to ensure complete download: /opt/***/models/base_model
[2025-04-17T03:26:05.933+0000] {Download_model.py:127} ERROR - Error downloading model: The weights trying to be saved contained shared tensors [{'model.layers.9.self_attn.q_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.weight'}, {'model.layers.20.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight'}, {'model.layers.19.mlp.gate_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight'}, {'model.layers.19.post_attention_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.29.input_layernorm.weight', 'model.norm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.20.input_layernorm.weight'}] that are mismatching the transformers base configuration. Try saving using `safe_serialization=False` or remove this tensor sharing.
[2025-04-17T03:26:05.936+0000] {Download_model.py:128} ERROR - Traceback (most recent call last):
  File "/opt/***/dags/Download_model.py", line 121, in download_model
    model.save_pretrained(f"{models_dir}/base_model", safe_serialization=True)
  File "/home/***/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3486, in save_pretrained
    raise RuntimeError(
RuntimeError: The weights trying to be saved contained shared tensors [{'model.layers.9.self_attn.q_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.weight'}, {'model.layers.20.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight'}, {'model.layers.19.mlp.gate_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight'}, {'model.layers.19.post_attention_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.29.input_layernorm.weight', 'model.norm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.20.input_layernorm.weight'}] that are mismatching the transformers base configuration. Try saving using `safe_serialization=False` or remove this tensor sharing.

[2025-04-17T03:26:05.937+0000] {taskinstance.py:1847} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 181, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 198, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/Download_model.py", line 121, in download_model
    model.save_pretrained(f"{models_dir}/base_model", safe_serialization=True)
  File "/home/airflow/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3486, in save_pretrained
    raise RuntimeError(
RuntimeError: The weights trying to be saved contained shared tensors [{'model.layers.9.self_attn.q_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.weight'}, {'model.layers.20.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight'}, {'model.layers.19.mlp.gate_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight'}, {'model.layers.19.post_attention_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.29.input_layernorm.weight', 'model.norm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.20.input_layernorm.weight'}] that are mismatching the transformers base configuration. Try saving using `safe_serialization=False` or remove this tensor sharing.
[2025-04-17T03:26:05.947+0000] {taskinstance.py:1368} INFO - Marking task as UP_FOR_RETRY. dag_id=zephyr_real_estate_model_dag, task_id=download_model, execution_date=20250417T032052, start_date=20250417T032059, end_date=20250417T032605
[2025-04-17T03:26:05.957+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 5 for task download_model (The weights trying to be saved contained shared tensors [{'model.layers.9.self_attn.q_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.weight'}, {'model.layers.20.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight'}, {'model.layers.19.mlp.gate_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight'}, {'model.layers.19.post_attention_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.29.input_layernorm.weight', 'model.norm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.20.input_layernorm.weight'}] that are mismatching the transformers base configuration. Try saving using `safe_serialization=False` or remove this tensor sharing.; 94)
[2025-04-17T03:26:05.979+0000] {local_task_job_runner.py:232} INFO - Task exited with return code 1
[2025-04-17T03:26:05.995+0000] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
